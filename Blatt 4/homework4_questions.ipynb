{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework4, Model Selection, Transformer (4 + 3 + 19 = 26 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Note:** You do not need to upload your transformer model checkpoint, it is too large.\n",
    "\n",
    "In this exercise, we are going to explore different model selection methods. Specifically, we are considering the Hold-out methods and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a toy data set: A third order polynomial where the training data is noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(33)\n",
    "n = 3\n",
    "x = np.linspace(-4, 2, 101)  # for plotting\n",
    "\n",
    "p_gt = [1, 3, -1, -4]  # coefficients for a third order polynome\n",
    "y_gt = np.polyval(p_gt, x)  # = p[0] * x**n + p[1] * x**(n-1) + ...\n",
    "\n",
    "n_data = 10  # create 10 data points\n",
    "x_data = np.random.uniform(-4, 2, n_data)\n",
    "y_data_gt = np.polyval(p_gt, x_data)\n",
    "y_data = y_data_gt + 1 * np.random.randn(n_data)\n",
    "\n",
    "plt.plot(x, y_gt, color='g', label=\"Ground truth f(x)\")\n",
    "plt.scatter(x_data, y_data, label=\"Noisy data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Polynomial Regression using Hold-Out Validation (4P)\n",
    "In this exercise, you will use the hold-out validation method to train and validate polynomial regression models of different orders. Your goal is to determine the optimal model complexity by minimizing the validation mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume the following data is provided:\n",
    "# x_data, y_data: arrays containing the input data points and corresponding noisy outputs\n",
    "# x: array of points for plotting the ground truth and best model\n",
    "# y_gt: array of ground truth values corresponding to x\n",
    "\n",
    "M = 20  # Maximum order of polynomials to train\n",
    "models = np.arange(1, M+1)\n",
    "MSEs = []\n",
    "mse_trains = []\n",
    "\n",
    "split = 0.7  # Proportion of data to use for training\n",
    "n_data = len(x_data)\n",
    "split_ind = int(split * n_data)\n",
    "\n",
    "# 1: Split into training and validation sets\n",
    "ind = np.random.choice(n_data, n_data, replace=False)  # Shuffle data indices\n",
    "ind_train = ind[:split_ind]\n",
    "ind_val = ind[split_ind:]\n",
    "D_T = {\"x\": x_data[ind_train], \"y\": y_data[ind_train]}  # Training data\n",
    "D_V = {\"x\": x_data[ind_val], \"y\": y_data[ind_val]}  # Validation data\n",
    "\n",
    "# Loop through polynomial orders and train models\n",
    "for m in models:\n",
    "    ########## TODO ##########\n",
    "    # 2: Train on training data\n",
    "    p = ... # (Hint: you can use np.polyfit)\n",
    "    ##########################\n",
    "\n",
    "    ########## TODO ##########\n",
    "    # Evaluate on validation data\n",
    "    MSE = ... # (Hint: you can use np.polyval)\n",
    "    MSEs.append(MSE)\n",
    "    ##########################\n",
    "\n",
    "    ########## TODO ##########\n",
    "    # Compute MSE on training data for comparison\n",
    "    mse_train = ...\n",
    "    mse_trains.append(mse_train)\n",
    "    ##########################\n",
    "\n",
    "    # Log parameters of the best model order\n",
    "    if MSE <= np.min(MSEs):\n",
    "        p_best = p\n",
    "\n",
    "########## TODO ##########\n",
    "# 4: Pick model with best validation loss\n",
    "m_star = ...\n",
    "print(\"Best model order: {}\".format(m_star))\n",
    "##########################\n",
    "\n",
    "# Plotting MSE for training and validation sets\n",
    "plt.figure()\n",
    "plt.semilogy(models, MSEs, label=\"Validation MSE\")\n",
    "plt.semilogy(models, mse_trains, label=\"Training MSE\")\n",
    "plt.xlabel(\"Polynomial Order\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the ground truth, data points, and best model\n",
    "plt.figure()\n",
    "plt.plot(x, y_gt, color='g', label=\"Ground truth\")\n",
    "plt.scatter(x_data, y_data, color='orange', label=\"Noisy data\")\n",
    "f_hat = np.polyval(p_best, x)\n",
    "plt.plot(x, f_hat, label=\"Best model\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Polynomial Regression using k-Fold Cross-Validation\n",
    "In this exercise, you will use the k-fold cross-validation method to train and validate polynomial regression models of different orders. Your goal is to determine the optimal model complexity by minimizing the mean squared error (MSE).\n",
    "\n",
    "### Exercise 2.1: Implement k-Fold Cross-Validation (2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(x, y, m, k):\n",
    "    \"\"\"\n",
    "    k-fold cross validation method\n",
    "    :param x: samples\n",
    "    :param y: targets\n",
    "    :param m: model order\n",
    "    :param k: number of partitions\n",
    "    :return mean MSE of test and training set\n",
    "    \"\"\"\n",
    "    n_data = y.size\n",
    "    if k > n_data:\n",
    "        k = n_data\n",
    "\n",
    "    MSEs_test = []\n",
    "    MSEs_train = []\n",
    "    \n",
    "    n_val_data = n_data // k  # how many data points are in validation set\n",
    "    rand_ind = np.random.choice(n_data, n_data, replace=False)  # random indices in [1, n] to shuffle data\n",
    "    \n",
    "    # loop over partitions\n",
    "    for k_ in range(k):\n",
    "\n",
    "        ########## TODO ##########\n",
    "        # 1: Split into training and validation dataset (Hint: You may use n_val_data and rand_int defined above the for-loop)\n",
    "        ind_val = ... # indices for validation data\n",
    "        ind_train = ... # indices for train data\n",
    "        ##########################\n",
    "        \n",
    "        # Training data set\n",
    "        D_T = {\"x\": x[ind_train],\n",
    "               \"y\": y[ind_train]}\n",
    "        \n",
    "        # Validation data set\n",
    "        D_V = {\"x\": x[ind_val],\n",
    "               \"y\": y[ind_val]}\n",
    "    \n",
    "        ########## TODO ##########\n",
    "        # 2: Train and evaluate again on training data to obtain estimator\n",
    "        # Hint: look at the hold out method and use built in numpy functions like polyfit and polyval\n",
    "        MSE_test = ...\n",
    "        MSE_train = ...\n",
    "        ##########################\n",
    "        \n",
    "        MSEs_test.append(MSE_test)\n",
    "        MSEs_train.append(MSE_train)\n",
    "        \n",
    "    return np.mean(MSEs_test), np.mean(MSEs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 20  # train polynomials up to order M\n",
    "models = np.arange(1, M+1)\n",
    "k = 10  # number of partitions, choose from 2 ... 10\n",
    "MSEs_test = []\n",
    "MSEs_train = []\n",
    "\n",
    "for m in models:\n",
    "    mse_test, mse_train = k_fold_cv(x_data, y_data, m, k)\n",
    "    MSEs_test.append(mse_test)\n",
    "    MSEs_train.append(mse_train)\n",
    "    \n",
    "m_star = models[np.argmin(MSEs_test)]\n",
    "print(\"Best model complexity:\", m_star)\n",
    "print(\"MSE:\", np.min(MSEs_test))\n",
    "plt.semilogy(models, MSEs_test, label=\"MSE test\")\n",
    "plt.semilogy(models, MSEs_train, label=\"MSE train\")\n",
    "plt.semilogy(m_star, np.min(MSEs_test), \"x\")\n",
    "plt.xticks(models)\n",
    "plt.xlabel(\"model order\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "\n",
    "p_cv = np.polyfit(x_data, y_data, m_star)\n",
    "y_cv = np.polyval(p_cv, x)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y_gt, color='g', label=\"Ground truth\")\n",
    "plt.scatter(x_data, y_data, label=\"Noisy data\", color=\"orange\")\n",
    "plt.plot(x, y_cv, label=\"Prediction\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Partition Sizes (1p)\n",
    "What is the influence of the number of partitions (folds) on the variance of the MSE? Why is that the case? Hint: Use the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3  # try different model orders\n",
    "partitions = np.arange(2, 10+1)\n",
    "MSEs_test_list = []\n",
    "MSEs_train_list = []\n",
    "MSEs_test_mean = []\n",
    "MSEs_train_mean = []\n",
    "MSEs_test_std = []\n",
    "MSEs_train_std = []\n",
    "\n",
    "# for different partition sizes, run k-fold cross validation 100 times\n",
    "# and record the mean and standard deviation of\n",
    "# the MSE of the train and test set\n",
    "for k in partitions:\n",
    "    for i in range(100):\n",
    "        mse_test_i, mse_train_i = k_fold_cv(x_data, y_data, m, k)\n",
    "        MSEs_test_list.append(mse_test_i)\n",
    "        MSEs_train_list.append(mse_train_i)\n",
    "    MSEs_test_mean.append(np.mean(MSEs_test_list))\n",
    "    MSEs_train_mean.append(np.mean(MSEs_train_list))\n",
    "    MSEs_test_std.append(np.std(MSEs_test_list))\n",
    "    MSEs_train_std.append(np.std(MSEs_train_list))\n",
    "    MSEs_test_list = []\n",
    "    MSEs_train_list = []\n",
    "    \n",
    "MSEs_test_mean = np.hstack(MSEs_test_mean)\n",
    "MSEs_train_mean = np.hstack(MSEs_train_mean)\n",
    "MSEs_test_std = np.hstack(MSEs_test_std)\n",
    "MSEs_train_std = np.hstack(MSEs_train_std)\n",
    "    \n",
    "plt.semilogy(partitions, MSEs_test_std, label=\"MSE test\")\n",
    "plt.semilogy(partitions, MSEs_train_std, label=\"MSE train\")\n",
    "\n",
    "plt.xticks(partitions)\n",
    "plt.xlabel(\"Number of partitions\")\n",
    "plt.ylabel(\"Variance of the MSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3, Transformer from Scratch (19 Points)\n",
    "In this task, you will implement a transformer model from scratch, following the original paper [\"Attention is All You Need\"](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). Then you will train it as a German to English translator. We will use the Multi30K dataset which contains 30k simple German-English sentence pairs. Let's firstly take a look of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import read_lines\n",
    "train_de = read_lines('data/multi30k/train.de')\n",
    "train_en = read_lines('data/multi30k/train.en')\n",
    "valid_de = read_lines('data/multi30k/valid.de')\n",
    "valid_en = read_lines('data/multi30k/valid.en')\n",
    "\n",
    "print(f\"German : {train_de[3]}\")\n",
    "print(f\"English: {train_en[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We need several pre-processing steps for the dataset, so that we can train our model on it. These steps include:\n",
    "1. Get the vocabulary dictionaries for both German and English\n",
    "2. Tokenize the sentences using the vocabulary dictionaries\n",
    "3. Create PyTorch datasets and dataloaders\n",
    "---\n",
    "To do so, we will use the pre-defined tokenizer from the library spacy (https://spacy.io/). We need to install it and download the German and English tokenizer models. You can do so by running the following commands. \n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download de_core_news_sm    # German tokenizer\n",
    "python -m spacy download en_core_web_sm     # English tokenizer\n",
    "```\n",
    "Some previous versions of `spacy` require that numpy's version <2.0 (if you run in to issues, downgrade numpy).\n",
    "---\n",
    "After that, you can use the following code to build the vocabularies dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import build_vocab, tokenize_de, tokenize_en\n",
    "vocab_de = build_vocab(train_de, tokenize_de)\n",
    "vocab_en = build_vocab(train_en, tokenize_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how vocabulary dictionary looks like. Try search the word \"Mannschafft\" in the German vocabulary dictionary and the \"football\" in the English dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Index of {'Mannschaft'} in the German vocabulary dictionary is {vocab_de['Mannschaft']}\")\n",
    "print(f\"Index of {'football'} in the English vocabulary dictionary is {vocab_en['football']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We also have 4 special tokens in the vocabulary dictionaries:\n",
    "1. \\<unk\\> for unknown words (words do not exist in spacy tokenizer)\n",
    "2. \\<pad\\> for padding words\n",
    "3. \\<bos\\> for the beginning of the sentence\n",
    "4. \\<eos\\> for the end of the sentence\n",
    "\n",
    "The padding is used when the length of a sentence is shorter than the maximum length of sentences in the mini-batch. For example, if we have 'Hello world.' and 'Machine learning is very cool.' in one mini-batch, we need to pad the shorter sentence 'Hello world.' with three padding tokens '\\<pad\\>' as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    0    |    1    |    2     | 3  |    4    |    5    |    6    |    7    |\n",
    "|:-------:|:-------:|:--------:|:--:|:-------:|:-------:|:-------:|:-------:|\n",
    "| \\<bos\\> |  Hello  |  world   | .  | \\<eos\\> | \\<pad\\> | \\<pad\\> | \\<pad\\> |\n",
    "| \\<bos\\> | Machine | learning | is |  very   |  cool   |    .    | \\<eos\\> |\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenize the sentences and decode it back, we can use the following python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences to tokens\n",
    "def encode(text, vocab, tokenizer):\n",
    "    return [vocab[\"<bos>\"]] + [\n",
    "        vocab[token] if token in vocab else vocab[\"<unk>\"] for token in\n",
    "        tokenizer(text)] + [vocab[\"<eos>\"]]\n",
    "\n",
    "# Tokens to sentences\n",
    "def decode(tokens, vocab):\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    return \" \".join([inv_vocab[token] for token in tokens if token not in (\n",
    "        vocab[\"<bos>\"], vocab[\"<eos>\"], vocab[\"<pad>\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the encode and decode functions work\n",
    "original_german_sentence = train_de[3]\n",
    "print(f\"Original German: {original_german_sentence}\")\n",
    "encoded_german_sentence = encode(original_german_sentence, vocab_de, tokenize_de)\n",
    "print(f\"Tokenized German: {encoded_german_sentence} \\n\")\n",
    "decoded_german_sentence = decode(encoded_german_sentence, vocab_de)\n",
    "print(f\"Decoded German: {decoded_german_sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To achieve the training of our German to English translator, we need to process each pair of German and English sentences in the dataset. We will use the following code to process the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Encode the German sentence and its corresponding English sentence\n",
    "def data_process(sentences_de, sentences_en):\n",
    "    data = []\n",
    "    for src_text, tgt_text in zip(sentences_de, sentences_en):\n",
    "        src_tensor_ = torch.tensor(encode(src_text, vocab_de, tokenize_de),\n",
    "                                   dtype=torch.long)\n",
    "        tgt_tensor_ = torch.tensor(encode(tgt_text, vocab_en, tokenize_en),\n",
    "                                   dtype=torch.long)\n",
    "        data.append((src_tensor_, tgt_tensor_))\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create the PyTorch dataset classes\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "# Get the PyTorch datasets instances\n",
    "train_dataset = TranslationDataset(data_process(train_de, train_en))\n",
    "valid_dataset = TranslationDataset(data_process(valid_de, valid_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "With the datasets in hand, now we implement our dataloader. Each time, the dataloader will return a mini-batch of the dataset. As each sentence in the mini-batch has a different length, we need a collate function to add padding tokens at the end of the short sentences. We will use the following code to implement the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# Collate function to pad short sentences \n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(src_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=vocab_de[\n",
    "        \"<pad>\"]).transpose(0, 1) # The transpose ensures batch first view\n",
    "    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=vocab_en[\n",
    "        \"<pad>\"]).transpose(0, 1)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(TranslationDataset(train_dataset), batch_size=32,\n",
    "                              shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(TranslationDataset(valid_dataset), batch_size=32,\n",
    "                              shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "OK, the data pre-processing part is finished. Now, let's take a look of the transformer architecture. The implementation has the following components:\n",
    "1. TokenEmbedding\n",
    "2. **Positional Encoding (your task)**\n",
    "3. **Multi-Head Attention (your task)**\n",
    "4. Feed Forward Neural Network\n",
    "5. Layer Normalization\n",
    "6. **Encoder Layer (your task)**\n",
    "7. **Decoder Layer (your task)**\n",
    "8. Encoder\n",
    "9. Decoder\n",
    "10. Transformer\n",
    "\n",
    "Please note, for consistency to previous homework, we choose to use the **batch first dimensionality manner** in the implementation. This means the shape of a sequence data is always **(batch size, sequence length, ...)**, rather than the one commonly used in natural linear processing(NLP), as (sequence length, batch size, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We begin with the TokenEmbedding. It is used to convert the token (vocabulary) index to the corresponding token embedding, namely a high dimensional feature vector (512 in the current homework). The implementation uses the build-in pytorch embedding class and takes the size of the vocabulary (token) dictionary and the size of the embedding. We follow the original paper [\"Attention is All You Need\"](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) page 5, to scale the embedding by the square root of the embedding size. The class has the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        \"\"\"\n",
    "            tokens: [batch_size, seq_len]\n",
    "            return: [batch_size, seq_len, emb_size]\n",
    "        \"\"\"\n",
    "        token_embed = self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "        return token_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1, Positional Encoding (4 Points)\n",
    "Next, we need to implement the positional encoding. The positional encoding is used to add the information about the position of the tokens in the sentence, which is later added to the token embedding. It is calculated as follows:\n",
    "\n",
    "$\\large   PE(t, 2l) = sin(\\frac{pos}{10000^{2l/d_{\\text{model}}}})  $\n",
    "\n",
    "$\\large   PE(t, 2l+1) = cos(\\frac{pos}{10000^{2l/d_{\\text{model}}}})$\n",
    "\n",
    "where $t$ is the positional index of the sequence and $l$ is the index of the embedding. The embedding's dimension size is equivalent to the model's dimension $d_{\\text{model}}$ of the transformer.\n",
    "In practice, positional encodings are computed in log-space and uses the exp-log trick to avoid numerical overflow, as:\n",
    "$\\large a^b = \\exp(\\log(a) \\times b)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, max_len: int = 5000):        \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Shape of the positional encoding: (max_len, emb_size)\n",
    "        self.encoding = torch.zeros(max_len, emb_size)\n",
    "        self.encoding.requires_grad = False  # No gradients needed\n",
    "        \n",
    "        ####################################################################\n",
    "        # TODO: Implement the positional encoding\n",
    "\n",
    "        ####################################################################\n",
    "        \n",
    "        # Add additional batch dimension in front, to (1, max_len, emb_size)\n",
    "        self.encoding = self.encoding.unsqueeze(0) \n",
    "\n",
    "    def forward(self, token_embed: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            token_embed: [batch_size, seq_len, emb_size]\n",
    "            pos_enc: [batch_size, seq_len, emb_size]\n",
    "        \"\"\"\n",
    "        seq_len = token_embed.size(1)\n",
    "        pos_enc = token_embed + self.encoding[:, :seq_len, :].to(token_embed.device)\n",
    "        return pos_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the positional encoding looks like. We also offer a reference image **positional_encoding.png** for you to compare and debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the positional encoding looks like, we also offer a reference image for you to compare.\n",
    "from util import show_positional_encoding\n",
    "show_positional_encoding(PositionalEncoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 2, Multi-Head Attention (5 Points)\n",
    "![Alt text](attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we need to implement the scaled dot-product attention and the multi-head attention.\n",
    "\n",
    "The attention mechanism can learn a mapping between a query (Q) and a set of key (K) value (V) pairs, where the Q, K, V are the projections from the hidden representation. We use three linear NN layers to model each projection respectively.\n",
    "The attention is achieved by\n",
    "Attention $\\large (Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$.\n",
    "In our programming, the shape of Q, K, V are:\n",
    "\n",
    " Q: [batch_size, seq_len_q, d_k],\n",
    "\n",
    " K: [batch_size, seq_len_k, d_k],\n",
    "\n",
    " V: [batch_size, seq_len_k, d_k],\n",
    "\n",
    "where d_k = d_model.\n",
    "\n",
    "<div style=\"border: 1px solid black; padding: 10px; border-radius: 5px; font-style: italic; font-size: 12px;\">\n",
    "    Comment: the meaning of these Q, K, V names can be confusing, because they depend on the particular Natural Language Processing (NLP) application. For developing our transformers you can just think of them as linear projections of the input.\n",
    "</div>\n",
    "\n",
    "<div style=\"border: 1px solid black; padding: 10px; border-radius: 5px; font-style: italic; font-size: 12px;\">\n",
    "    Comment: In transformers, when Q comes from the same sequence as K and V, it is called self-attention. When K and V are from a different sequence than Q, it is called cross-attention.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Additionally, we can have multiple such attention heads, to achieve the multi-head attention, with each head having its own set of Q, K, V parameters. The outputs of these multiple heads are concatenated and linearly transformed to produce the final output. To achieve this, we add one additional dimension to the Q, K, V tensors, which is the number of heads. The data dimension thus get reduced to d_k = d_model / n_head\n",
    "\n",
    "The shape of the Q, K, V in the multi-head attention are:\n",
    "\n",
    " Q: [batch_size, n_head, seq_len_q, d_k],\n",
    "\n",
    " K: [batch_size, n_head, seq_len_k, d_k],\n",
    "\n",
    " V: [batch_size, n_head, seq_len_k, d_k].\n",
    "\n",
    "<div style=\"border: 1px solid black; padding: 10px; border-radius: 5px; font-style: italic; font-size: 12px;\">\n",
    "    Comment: we can consider each attention head in Transformer as a channel used in the CNN, capturing a certain pattern in the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import merge_masks\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, n_head: int = 8, dropout: float = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_head == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_model // n_head\n",
    "        self.q_net = nn.Linear(d_model, d_model)\n",
    "        self.k_net = nn.Linear(d_model, d_model)\n",
    "        self.v_net = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, causal_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "            q: [batch_size, seq_len_q, d_model]\n",
    "            k: [batch_size, seq_len_k, d_model]\n",
    "            v: [batch_size, seq_len_k, d_model]\n",
    "            causal_mask: [seq_len_q, seq_len_k]\n",
    "            padding_mask: [batch_size, seq_len_k]\n",
    "            return: [batch_size, seq_len_q, d_model]\n",
    "\n",
    "            if self attention, q, k, v are the same, and seq_len_q = seq_len_k\n",
    "            if cross attention, q != k = v, and seq_len_q != seq_len_k\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        # We need some special care to merge the masks. The mask_ij basically\n",
    "        # tells if the i-th token in query is allowed to attend to the j-th\n",
    "        # token in key.\n",
    "        # The shape of mask is [batch_size, n_head, len_q, len_k] or any shape\n",
    "        # that can be broadcasted to this shape\n",
    "        mask = merge_masks(causal_mask, padding_mask, self.n_head)\n",
    "\n",
    "        # Linear projections\n",
    "        q, k, v = self.q_net(q), self.k_net(k), self.v_net(v)\n",
    "\n",
    "        ####################################################################\n",
    "        # TODO: Implement the multi-head attention\n",
    "        # Consider the following steps:\n",
    "        # 1. Split the Q, K, V into n_head by using torch.view\n",
    "        # 2. Put the head axis after the batch dimension using torch.transpose\n",
    "        # 3. Calculate the attention scores using Q, K, V\n",
    "        # 4. Apply the mask to the attention scores by using torch.masked_fill\n",
    "        # 5. Apply the softmax function to the attention scores\n",
    "        # 6. Apply the dropout to the softmax scores to avoid overfitting\n",
    "        # 7. Multiply the softmax scores with V\n",
    "\n",
    "\n",
    "\n",
    "        ####################################################################\n",
    "\n",
    "        # Concatenate and multiply get processed by the output layer\n",
    "        # [batch_size, n_head, T, d_k]\n",
    "        # -> [batch_size, T, n_head, d_k]\n",
    "        # -> [batch_size, T, n_head * d_k], where n_head * d_k = d_model\n",
    "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.output_linear(attn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "<img src=\"ff.png\" alt=\"Alt text\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed-forward neural network is used to transform the output of the multi-head attention. It consists of two linear layers with a ReLU activation in between. The implementation has the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "https://www.pinecone.io/learn/batch-layer-normalization/\n",
    "<div style=\"display: flex; flex-direction: row;\">\n",
    "    <div>\n",
    "        <img src=\"layer_norm.webp\" alt=\"Image 2\" style=\"width: 300px;\"/>\n",
    "    </div>\n",
    "    <div style=\"margin-right: 10px;\">\n",
    "        <img src=\"batch_norm.webp\" alt=\"Image 1\" style=\"width: 350px;\"/>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer architecture uses a layer normalization (left) instead of a batch normalization (right), which is often used in Computer Vision.\n",
    "The layer normalization is to normalize the input tensor across the feature dimension, which is the last dimension of the tensor.\n",
    "\n",
    "$y = \\gamma ( \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} ) + \\beta$\n",
    "\n",
    "The implementation has the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to torch.nn.LayerNorm(d_model)\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3, Encoder Layer (4 Points)\n",
    "<img src=\"encoder_layer.jpg\" alt=\"Alt text\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder layer consists of one self-attention layer (an instance of MultiHeadAttention) and one feed-forward neural network. The output of each sub-layer is LayerNorm(x + dropout(Sublayer(x))), where Sublayer(x) is the forward function implemented by the sub-layer (MultiHeadAttention or Feedforward) itself.\n",
    "<div style=\"border: 1px solid black; padding: 10px; border-radius: 5px; font-style: italic; font-size: 12px;\">\n",
    "    Comment: the lecture slides swapped the order of layer norm and the residual connection. We will update it to stay the same as the original paper.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_head, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, causal_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "            src: [batch_size, seq_len, d_model]\n",
    "            causal_mask: [seq_len, seq_len]\n",
    "            padding_mask: [batch_size, seq_len]\n",
    "            return: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        ####################################################################\n",
    "        # TODO: Implement the encoder layer\n",
    "\n",
    "        ####################################################################\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 4, Decoder Layer (6 Points)\n",
    "<img src=\"decoder_layer.jpg\" alt=\"Alt text\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder layer consists of three sub-layers: self-attention, cross-attention, and feed-forward neural network. The output of each sub-layer is LayerNorm(x + dropout(Sublayer(x))), where Sublayer(x) is the function implemented by the sub-layer itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_head)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_padding_mask=None, memory_padding_mask=None):\n",
    "        \"\"\"\n",
    "            tgt: [batch_size, seq_len_tgt, d_model]\n",
    "            memory: [batch_size, seq_len_src, d_model]\n",
    "            tgt_mask: [seq_len_tgt, seq_len_tgt]\n",
    "            memory_mask: [seq_len_src, seq_len_src]\n",
    "            tgt_padding_mask: [batch_size, seq_len_tgt]\n",
    "            memory_padding_mask: [batch_size, seq_len_src]\n",
    "            return: [batch_size, seq_len, d_model]\"\"\"\n",
    "\n",
    "        ####################################################################\n",
    "        # TODO: Implement the decoder layer\n",
    "        # Consider the following steps:\n",
    "        # 1. Calculate the self-attention of the target\n",
    "        # 2. Add the residual connection, dropout, and apply the layer normalization\n",
    "        # 3. Calculate the cross-attention of the target with the memory\n",
    "        # 4. Add the residual connection, dropout, and apply the layer normalization\n",
    "        # 5. Apply the feed-forward neural network\n",
    "        # 6. Add the residual connection, dropout, and apply the layer normalization\n",
    "\n",
    "\n",
    "        ####################################################################\n",
    "        return tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "Encoder is composed of a stack of N identical encoder layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_head, n_layers, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, n_head, d_ff, dropout)\n",
    "             for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, src, mask, padding_mask):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, mask, padding_mask)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "Decoder is composed of a stack of N identical decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, n_head, n_layers, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, n_head, d_ff, dropout)\n",
    "             for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask, memory_mask,\n",
    "                tgt_padding_mask, memory_padding_mask):\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask, memory_mask,\n",
    "                        tgt_padding_mask, memory_padding_mask)\n",
    "        return tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "Transformer is composed of the encoder and decoder. We exclude the final linear and softmax layer from the original transformer architecture, so that it stays the same as PyTorch's build-in Transformer implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, n_head,\n",
    "                 num_encoder_layers, num_decoder_layers, d_ff):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, n_head, num_encoder_layers, d_ff)\n",
    "        self.decoder = Decoder(d_model, n_head, num_decoder_layers, d_ff)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, memory_mask,\n",
    "                src_padding_mask, tgt_padding_mask, memory_padding_mask\n",
    "                ):\n",
    "        memory = self.encoder(src, src_mask, src_padding_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask, memory_mask,\n",
    "                              tgt_padding_mask, memory_padding_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Put Everything Together\n",
    "We build out transformer translator model by combining the TokenEmbedding, PositionalEncoding, Transformer, and the final linear layer. We define our loss function as the cross-entropy loss. We also define a function to compute the loss of the model on the a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTranslator(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, emb_size, nhead,\n",
    "                 num_encoder_layers, num_decoder_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer \\\n",
    "            = Transformer(d_model=emb_size, n_head=nhead,\n",
    "                          num_encoder_layers=num_encoder_layers,\n",
    "                          num_decoder_layers=num_decoder_layers,\n",
    "                          d_ff=dim_feedforward)\n",
    "        \n",
    "        # If you cannot make your model work, here is the PyTorch's build-in Transformer\n",
    "        #self.transformer \\\n",
    "        #    = torch.nn.Transformer(d_model=emb_size, nhead=nhead,\n",
    "        #                  num_encoder_layers=num_encoder_layers,\n",
    "        #                  num_decoder_layers=num_decoder_layers,\n",
    "        #                  dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask,\n",
    "                tgt_padding_mask, memory_padding_mask):\n",
    "        # Get token embeding and positional encoding\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "\n",
    "        outs = self.transformer(src=src_emb, tgt=tgt_emb, src_mask=src_mask,\n",
    "                                tgt_mask=tgt_mask, memory_mask=None,\n",
    "                                src_padding_mask=src_padding_mask,\n",
    "                                tgt_padding_mask=tgt_padding_mask,\n",
    "                                memory_padding_mask=memory_padding_mask)\n",
    "\n",
    "        return self.generator(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import create_mask\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_en['<pad>'])\n",
    "\n",
    "# Determine your device. You can use a GPU if it is available, otherwise, use the CPU\n",
    "# \"mps\" for MacOS GPU, but your TA does not have a Mac. You may need to try it yourself...\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "def compute_loss(model, data_loader, criterion, optimizer=None):\n",
    "    if optimizer:\n",
    "        # train mode\n",
    "        model.train() # train mode: enable dropout\n",
    "    else:\n",
    "        # eval mode\n",
    "        model.eval()  # eval mode: disable dropout\n",
    "    total_loss = 0\n",
    "    for src, tgt in data_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        # We need to create masks for correct attention behaviour\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "            src, tgt_input, vocab_de, vocab_en, device)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask,\n",
    "                       tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]),\n",
    "                         tgt_out.reshape(-1))\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our training loop. We use the Adam optimizer with learning rate 1e-4. We train the model for 10 epochs (10min for GPU, 1-2 hours for CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = TransformerTranslator(len(vocab_de), len(vocab_en), 512, 8, 6, 6, 512).to(\n",
    "    device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    train_loss = compute_loss(model, train_dataloader, criterion, optimizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valid_loss = compute_loss(model, valid_dataloader, criterion, optimizer=None)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "\n",
    "        torch.save(model, 'model/model_best.pth')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {train_loss}, Val_loss: {valid_loss}')\n",
    "torch.save(model, 'model/model_last.pth')\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model training, we can test our model's performance, by translating some German sentences to English. We need a helper function to work with our transformer model, so that the predicted English word can be used again to generate the next word. The following code snippet shows how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import generate_square_subsequent_mask\n",
    "import time\n",
    "\n",
    "# Function to translate sentences\n",
    "def translate_sentence(sentence, model, vocab_de, vocab_en, tokenizer_de,\n",
    "                       max_length=50):\n",
    "    model.eval()\n",
    "    src = torch.tensor(encode(sentence, vocab_de, tokenizer_de),\n",
    "                       dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # The translation starts with token <bos>\n",
    "    tgt = torch.tensor([vocab_en[\"<bos>\"]], dtype=torch.long).unsqueeze(0).to(\n",
    "        device)\n",
    "\n",
    "    src_mask = torch.zeros((src.shape[1], src.shape[1]), device=device).type(\n",
    "        torch.bool)\n",
    "\n",
    "    memory = model.transformer.encoder(\n",
    "        model.positional_encoding(model.src_tok_emb(src)), src_mask, None)\n",
    "\n",
    "    for i in range(max_length):        \n",
    "        tgt_mask = generate_square_subsequent_mask(tgt.size(1), device)\n",
    "\n",
    "        out = model.transformer.decoder(\n",
    "            model.positional_encoding(model.tgt_tok_emb(tgt)), memory, tgt_mask, None, None, None)\n",
    "        out = model.generator(out[:, -1])\n",
    "        prob = out.softmax(dim=-1)\n",
    "        next_word = prob.argmax(dim=-1).item()\n",
    "\n",
    "        # We append the predicted word to the output sentence\n",
    "        tgt = torch.cat([tgt, torch.tensor([[next_word]], device=device)],\n",
    "                        dim=1)\n",
    "        \n",
    "        # The translation ends with token <eos>\n",
    "        if next_word == vocab_en[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    return decode(tgt.squeeze().tolist(), vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of German sentences\n",
    "german_list = [\n",
    "    \"Eine junge Frau sitzt auf einer Mauer und schaut aufs Meer.\",\n",
    "    \"Mehrere Kinder rennen über einen Spielplatz.\",\n",
    "    \"Ein älterer Mann geht mit seinem Hund im Park spazieren.\",\n",
    "    \"Die Familie sitzt auf der Veranda und isst Frühstück.\",\n",
    "    \"Eine Gruppe von Touristen besichtigt eine historische Burg.\",\n",
    "    \"Ein Mädchen pflückt Blumen in einem blühenden Garten.\",\n",
    "    \"Der Musiker spielt ein Lied auf seiner Geige in der Fußgängerzone.\",\n",
    "    \"Die Katze liegt faul auf dem Fensterbrett und sonnt sich.\",\n",
    "    \"Das Paar fährt mit dem Fahrrad durch die malerische Landschaft.\",\n",
    "    \"Der Koch bereitet ein festliches Abendessen in der Küche vor.\",\n",
    "    \"Eine Menschenmenge schaut einem Straßenkünstler zu.\",\n",
    "    \"Die Kinder bauen eine Sandburg am Strand.\",\n",
    "    \"Der Maler arbeitet an einem neuen Kunstwerk in seinem Atelier.\",\n",
    "    \"Eine Frau liest ein Buch unter einem schattigen Baum.\",\n",
    "    \"Der Hund jagt einen Ball über die Wiese.\",\n",
    "    \"Eine Gruppe von Freunden zeltet in den Bergen.\",\n",
    "    \"Der Fotograf macht Bilder von der Skyline der Stadt.\",\n",
    "    \"Die Tänzer üben eine Choreografie im Studio.\",\n",
    "    \"Der Bauer fährt mit dem Traktor über sein Feld.\",\n",
    "    \"Die Schülerin macht ihre Hausaufgaben am Schreibtisch.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Load the model best or last\n",
    "# use_model = torch.load('model/model_best.pth', weights_only=False)\n",
    "use_model = torch.load(\"model/model_last.pth\", weights_only=False)\n",
    "\n",
    "# Translate each sentence in the list\n",
    "for german_sentence in german_list:\n",
    "    translation = translate_sentence(german_sentence, use_model, vocab_de, vocab_en,\n",
    "                                     tokenize_de)\n",
    "    print(f\"German: {german_sentence}\")\n",
    "    print(f\"English: {translation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are you happy with the performance?\n",
    "We offered a reference result in the \"reference_result.png\" for your comparison. Clearly the translation is not perfect. The model can learn the basic structure of the sentences, but it may not be able to capture the exact meaning. The model can be further improved by using a larger dataset, a larger model, a better structure or finetuning the hyper-parameters. If the sentence is beyond the data distribution, then the model may not be able to translate it at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_german_list = [\"Die deutsche Fußball-Nationalmannschaft ist mit einem überzeugenden Sieg in die Fußball-Europameisterschaft gestartet.\",\n",
    "                    \"Die Auswahl von Bundestrainer Julian Nagelsmann gewann am Freitagabend in München mit 5:1 (3:0) gegen Schottland.\"]\n",
    "\n",
    "for german_sentence in hard_german_list:\n",
    "    translation = translate_sentence(german_sentence, use_model, vocab_de, vocab_en,\n",
    "                                     tokenize_de)\n",
    "    print(f\"German: {german_sentence}\")\n",
    "    print(f\"English: {translation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
