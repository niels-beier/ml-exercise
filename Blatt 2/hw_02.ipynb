{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2 - ML - Grundverfahren\n",
    "\n",
    "Denis Blessing: denis.blessing@kit.edu. This homework has 4 topics with 19 points in total.\n",
    "\n",
    "## Submission Instructions\n",
    "Please follow the instruction from homework ZERO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Multiclass Classification (7p)\n",
    "\n",
    "The Iris Dataset is a very classical machine learning and statistics benchmark for classification, developed in the 1930's. The goal is to classify 3 types of flowers (more specifically, 3 types of flowers form the Iris species) based on 4 features: petal length, petal width, sepal length and sepal width.\n",
    "\n",
    "As we have $K=3$ different types of flowers we are dealing with a multi-class classification problem and need to extend our sigmoid-based classifier from the previous exercise / recap session.\n",
    "\n",
    "We will reuse our \"minimize\" and \"affine feature\" functions. Those are exactly as before. The affine features are sufficient here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def minimize(f: Callable , df: Callable, x0: np.ndarray, lr: float, num_iters: int) -> \\\n",
    "        Tuple[np.ndarray, float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    :param f: objective function\n",
    "    :param df: gradient of objective function\n",
    "    :param x0: start point, shape [dimension]\n",
    "    :param lr: learning rate\n",
    "    :param num_iters: maximum number of iterations\n",
    "    :return argmin, min, values of x for all interations, value of f(x) for all iterations\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    x = np.zeros([num_iters + 1] + list(x0.shape))\n",
    "    f_x = np.zeros(num_iters + 1)\n",
    "    x[0] = x0\n",
    "    f_x[0] = f(x0)\n",
    "    for i in range(num_iters):\n",
    "        # update using gradient descent rule\n",
    "        grad = df(x[i])\n",
    "        x[i + 1] = x[i] - lr * grad\n",
    "        f_x[i + 1] = f(x[i + 1])\n",
    "    return x[i+1], f_x[i+1], x[:i+1], f_x[:i+1] # logging info for visualization\n",
    "\n",
    "\n",
    "def affine_features(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    implements affine feature function\n",
    "    :param x: inputs\n",
    "    :return inputs with additional bias dimension\n",
    "    \"\"\"\n",
    "    return np.concatenate([x, np.ones((x.shape[0], 1))], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Data\n",
    "In the original dataset the different types of flowers are labeled with $0, 1$ and $2$. The output of our classifier will be a vector with $K=3$ entries, $\\begin{pmatrix}p(c=0 | \\boldsymbol x) & p(c=1 | \\boldsymbol x) & p(c=2 | \\boldsymbol x) \\end{pmatrix}$, i.e. the probability for each class that a given sample is an instance of that class, given a datapoint $\\boldsymbol x$. As presented in the lecture, working with categorical (=multinomial) distributions is easiest when we represent the labels in a different form, a so called one-hot encoding. This is a vector of the length of number of classes, in this case 3, with zeros everywhere except for the entry corresponding to the class number, which is one. For the train and test data we know to which class it belongs, so the probability for that class is one and the probability for all other classes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"iris_data.npz\")\n",
    "train_samples = data[\"train_features\"]\n",
    "train_labels = data[\"train_labels\"]\n",
    "test_samples = data[\"test_features\"]\n",
    "test_labels = data[\"test_labels\"]\n",
    "\n",
    "train_features = affine_features(train_samples)\n",
    "test_features = affine_features(test_samples)\n",
    "\n",
    "def generate_one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param y: vector containing classes as numbers, shape: [N]\n",
    "    :param num_classes: number of classes\n",
    "    :return a matrix containing the labels in an one-hot encoding, shape: [N x K]\n",
    "    \"\"\"\n",
    "    y_oh = np.zeros([y.shape[0], num_classes])\n",
    "\n",
    "    # can be done more efficiently using numpy with\n",
    "    # y_oh[np.arange(y.size), y] = 1.0\n",
    "    # we use the for loop for clarity\n",
    "\n",
    "    for i in range(y.shape[0]):\n",
    "        y_oh[i, y[i]] = 1.0\n",
    "\n",
    "    return y_oh\n",
    "\n",
    "\n",
    "oh_train_labels = generate_one_hot_encoding(train_labels, 3)\n",
    "oh_test_labels = generate_one_hot_encoding(test_labels, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization using Gradient Descent\n",
    "\n",
    "The multi-class generalization of the sigmoid is the softmax function. It takes an vector of length $K$ and outputs another vector of length $K$ where the $k$-th entry is given by\n",
    "$$ \\textrm{softmax}(\\boldsymbol{x})_k = \\dfrac{\\exp(x_k)}{\\sum_{j=1}^K \\exp(x_j)}.$$\n",
    "This vector contains positive elements which sum to $1$ and thus can be interpreted as parameters of a categorical distribution. Lets see how we can implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"softmax function\n",
    "    :param x: inputs, shape: [N x K]\n",
    "    :return softmax(x), shape [N x K]\n",
    "    \"\"\"\n",
    "    a = np.max(x, axis=-1, keepdims=True)\n",
    "    log_normalizer = a + np.log(np.sum(np.exp(x - a), axis=-1, keepdims=True))\n",
    "    return np.exp(x - log_normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Aspect:** In the above implementation of the softmax we stayed in the log-domain until the very last command.\n",
    "We also used the log-sum-exp-trick (https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations).\n",
    "Staying in the log domain and applying the log-sum-exp-trick whenever possible is a simple way to make the implementation\n",
    "numerically more robust. It does not change anything with regards to the underlying theory.\n",
    "\n",
    "We also need to extend our loss function. Instead of the log-likelihood of a Bernoulli distribution, we now maximize the log-likelihood of a categorical distribution which, for a single sample $\\boldsymbol{x}_i$, is given by\n",
    "$$\\log p(c_i | \\boldsymbol x_i) = \\sum_{k=1}^K h_{i, k} \\log(p_{i,k})$$\n",
    "where $\\boldsymbol h_i$ denotes the one-hot encoded true label and $p_{i,k} \\equiv p(c_i = k | \\boldsymbol x_i)$ the class probabilities predicted by the classifier. In multiclass classification, we learn one weight vector $\\boldsymbol w_k$ per class s.t. those probabilities are given by $p(c_i = k | \\boldsymbol x_i) = \\mathrm{softmax}(\\boldsymbol w^T \\boldsymbol \\phi (\\boldsymbol x_i))_k $.\n",
    "We can now implement the (negative) log-likelihood of a categorical distribution (we use the negative log-likelihood as we will minimize the loss later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_nll(predictions: np.ndarray, labels: np.ndarray, epsilon: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    cross entropy loss function\n",
    "    :param predictions: class labels predicted by the classifier, shape: [N x K]\n",
    "    :param labels: true class labels, shape: [N x K]\n",
    "    :param epsilon: small offset to avoid numerical instabilities (i.e log(0))\n",
    "    :return negative log-likelihood of the labels given the predictions, shape: [N]\n",
    "    \"\"\"\n",
    "    return - np.sum(labels * np.log(predictions + epsilon), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the loss for a single sample. To get the loss for all samples we will need to sum over loss for a single sample\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal L_{\\mathrm{cat-NLL}}\n",
    "=& - \\sum_{i=1}^N \\log p(c_i | \\boldsymbol x_i) \\\\\n",
    "=& - \\sum_{i=1}^N \\sum_{k=1}^K h_{i, k} \\log(p_{i,k}) \\\\\n",
    "=& - \\sum_{i=1}^N  \\sum_{k=1}^K h_{i, k} \\log(\\textrm{softmax}(\\boldsymbol{w}_k^T \\boldsymbol \\phi(\\boldsymbol{x}_i))_k)\\\\\n",
    "=& - \\sum_{i=1}^N \\left(\\sum_{k=1}^K h_{i,k}\\boldsymbol{w}^T_k \\boldsymbol \\phi(\\boldsymbol{x}_i) - \\log \\sum_{j=1}^K \\exp(\\boldsymbol{w}_j^T \\boldsymbol \\phi(\\boldsymbol{x}_i))\\right).\n",
    "\\end{align}\n",
    "\n",
    "In order to use gradient based optimization for this, we of course also need to derive the gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1) Derivation (4 Points)\n",
    "Derive the gradient $\\dfrac{\\partial \\mathcal L_{\\mathrm{cat-NLL}}}{\\partial \\boldsymbol{w}}$ of the loss function w.r.t. the full weight vector $\\boldsymbol w \\equiv \\begin{pmatrix} \\boldsymbol w_1^T & \\dots & \\boldsymbol w_K^T \\end{pmatrix}^T$, which is obtained by stacking the class-specific weight vectors $\\boldsymbol w_k$.\n",
    "\n",
    "**Hint 1:** Follow the steps in the derivation of the gradient of the loss for the binary classification in the lecture.\n",
    "\n",
    "**Hint 2:** Derive the gradient not for the whole vector $\\boldsymbol w$ but only for $\\boldsymbol w_k$, i.e., $\\dfrac{\\partial \\mathcal L_{\\mathrm{cat-NLL}}}{\\partial \\boldsymbol{w}_k}$. The gradients for the individual\n",
    "$\\boldsymbol w_k$ can then be stacked to obtain the full gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2) Implementation (3 Points)\n",
    "Now that we have the formulas for the loss and its gradient, we can implement them. Fill in the function skeletons below so that they implement the loss and its gradient. Again, in praxis, it is advisable to work with the mean nll instead of the sum, as this simplifies setting the learning rate.\n",
    "\n",
    "**Hint:** The optimizer works with vectors only. So the function get the weights as vectors in the flat_weights parameter.\n",
    "Make sure you use efficient vectorized computations (no for-loops!). Thus, we reshape the weights appropriately before using them for the computations. For the gradients make sure to return\n",
    "again a vector by flattening the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective\n",
    "def objective_cat(flat_weights: np.ndarray, features: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    :param flat_weights: weights of the classifier (as flattened vector), shape: [feature_dim * K]\n",
    "    :param features: samples to evaluate objective on, shape: [N x feature_dim]\n",
    "    :param labels: labels corresponding to samples, shape: [N x K]\n",
    "    :return cross entropy loss of the classifier given the samples\n",
    "    \"\"\"\n",
    "    num_features = features.shape[-1]\n",
    "    num_classes = labels.shape[-1]\n",
    "    weights = np.reshape(flat_weights, [num_features, num_classes])\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# derivative\n",
    "def d_objective_cat(flat_weights: np.ndarray, features: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param flat_weights: weights of the classifier (as flattened vector), shape: [feature_dim * K]\n",
    "    :param features: samples to evaluate objective on, shape: [N x feature_dim]\n",
    "    :param labels: labels corresponding to samples, shape: [N x K]\n",
    "    :return gradient of cross entropy loss of the classifier given the samples, shape: [feature_dim * K]\n",
    "    \"\"\"\n",
    "    feature_dim = features.shape[-1]\n",
    "    num_classes = labels.shape[-1]\n",
    "    weights = np.reshape(flat_weights, [feature_dim, num_classes])\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO, do not forget to flatten the gradient before returning!\n",
    "    #---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can tie everything together again. Both train and test accuracy should be at least 0.9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# optimization\n",
    "\n",
    "w0_flat = np.zeros(5 * 3) # 4 features + bias, 3 classes\n",
    "w_opt_flat, loss_opt, x_history, f_x_history = \\\n",
    "   minimize(lambda w: objective_cat(w, train_features, oh_train_labels),\n",
    "            lambda w: d_objective_cat(w, train_features, oh_train_labels),\n",
    "            w0_flat, 1e-2, 1000)\n",
    "\n",
    "w_opt = np.reshape(w_opt_flat, [5, 3])\n",
    "\n",
    "# plotting and evaluation\n",
    "print(\"Final Loss:\", loss_opt)\n",
    "plt.figure()\n",
    "plt.plot(f_x_history)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"negative categorical log-likelihood\")\n",
    "\n",
    "train_pred = softmax(train_features @ w_opt)\n",
    "train_acc = np.count_nonzero(np.argmax(train_pred, axis=-1) == np.argmax(oh_train_labels, axis=-1))\n",
    "train_acc /= train_labels.shape[0]\n",
    "test_pred = softmax(test_features @ w_opt)\n",
    "test_acc = np.count_nonzero(np.argmax(test_pred, axis=-1) == np.argmax(oh_test_labels, axis=-1))\n",
    "test_acc /= test_labels.shape[0]\n",
    "print(\"Train Accuracy:\", train_acc, \"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Feature-Based Support Vector Machine (Hinge Loss) (5 Points)\n",
    "In this exercise, we will train a feature-based SVM on the two moons dataset using the hinge loss. We will use the L-BFGS-B algorithm, provided by `scipy.optimize` for the optimization. All you need to know about this optimizer is that it is gradient-based. Otherwise, you can treat it as a black box. Yet, it's also worth a closer look if you are interested. To install scipy, use: **conda install -c conda-forge scikit-learn**\n",
    "\n",
    "We load and visualize the data again. Note we change the label for class 0 to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "\n",
    "## load data\n",
    "train_data = dict(np.load(\"two_moons.npz\", allow_pickle=True))\n",
    "train_samples = train_data[\"samples\"]\n",
    "train_labels = train_data[\"labels\"]\n",
    "# we need to change the labels for class 0 to -1 to account for the different labels used by an SVM\n",
    "train_labels[train_labels == 0] = -1\n",
    "\n",
    "test_data = dict(np.load(\"two_moons_test.npz\", allow_pickle=True))\n",
    "test_samples = test_data[\"samples\"]\n",
    "test_labels = test_data[\"labels\"]\n",
    "# we need to change the labels for class 0 to -1 to account for the different labels used by an SVM\n",
    "test_labels[test_labels == 0] = -1\n",
    "\n",
    "## plot data\n",
    "plt.figure(figsize = (12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.grid(\"on\")\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.ylim(-1, 1.5)\n",
    "plt.title(\"Train Data\")\n",
    "plt.scatter(x=train_samples[train_labels == -1, 0], y=train_samples[train_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "plt.scatter(x=train_samples[train_labels == 1, 0], y=train_samples[train_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.grid(\"on\")\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.ylim(-1, 1.5)\n",
    "plt.title(\"Test Data\")\n",
    "plt.scatter(x=test_samples[test_labels == -1, 0], y=test_samples[test_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "plt.scatter(x=test_samples[test_labels == 1, 0], y=test_samples[test_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we use cubic features $\\phi(\\boldsymbol x)$. Let us define a function to compute those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cubic_feature_fn(samples: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param x: Batch of 2D data vectors [x, y] [N x dim]\n",
    "    :return cubic features: [x**3, y**3, x**2 * y, x * y**2, x**2, y**2, x*y, x, y, 1]\n",
    "    \"\"\"\n",
    "    x = samples[..., 0]\n",
    "    y = samples[..., 1]\n",
    "    return np.stack([x**3, y**3, x**2 * y, x * y**2, x**2, y**2, x*y, x, y, np.ones(x.shape[0])], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 2.1) Hinge Loss Objective (2 Points)\n",
    "\n",
    "We will implement the hinge loss objective in this exercise. Its given by\n",
    " $$\\mathcal{L}_{\\boldsymbol{X}, \\boldsymbol{y}}(\\boldsymbol w) = \\parallel \\boldsymbol{w} \\parallel^2 + C \\sum_i^N  \\max \\left( 0, 1 - y_i \\boldsymbol{w}^T \\phi(\\boldsymbol{x}_i) \\right),$$\n",
    " where $\\boldsymbol{w}$ are our model parameters, $\\phi(\\boldsymbol{x})$ are our cubic features and the $y_i \\in \\lbrace{-1, 1\\rbrace}$ are the class labels.\n",
    "\n",
    " The following function implements the hinge loss. Fill in the missing code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def objective_svm(weights: np.ndarray, features: np.ndarray, labels: np.ndarray, slack_regularizer: float) -> float:\n",
    "    \"\"\"\n",
    "    objective for svm training with hinge loss\n",
    "    :param weights: current weights to evaluate (shape: [feature_dim])\n",
    "    :param features: features of training samples (shape:[N x feature_dim])\n",
    "    :param labels: class labels corresponding to train samples (shape: [N])\n",
    "    :param slack_regularizer: factor to weigh the violation of margin with (C in slides)\n",
    "    :returns svm (hinge) objective (shape: scalar)\n",
    "    \"\"\"\n",
    "    ### TODO ###############################\n",
    "    \n",
    "    ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2) Hinge Loss Gradient (3 Points)\n",
    "Derive and implement the gradient for the hinge loss objective stated above. For all non-differentiable points in the loss curves, you can use any valid subgradient.\n",
    "\n",
    "**NOTE**: The derivation is explicitly part of this exercise, so state it in the file, not just implement it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def d_objective_svm(weights: np.ndarray, features: np.ndarray, labels: np.ndarray, slack_regularizer: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    gradient of objective for svm training with hinge loss\n",
    "    :param weights: current weights to evaluate (shape: [feature_dim])\n",
    "    :param features: features of training samples (shape: [N x feature_dim])\n",
    "    :param labels: class labels corresponding to train samples (shape: [N])\n",
    "    :param slack_regularizer: factor to weigh the violation of margin with (C in slides)\n",
    "    :returns gradient of svm objective (shape: [feature_dim])\n",
    "    \"\"\"\n",
    "    ### TODO ###############################\n",
    "    \n",
    "    ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "Finally, we can tie everything together and get our maximum margin classifier. Here we are using the L-BFGS-B optimizer provided by Scipy. With $C=1000$ you should end up at a training accuracy of 1 and test accuracy of 0.99, but feel free to play around with $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_fn = cubic_feature_fn\n",
    "C = 1000.0\n",
    "\n",
    "## optimization (for details cf. https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html)\n",
    "train_features = feature_fn(train_samples)\n",
    "res = opt.minimize(\n",
    "    # pass objective\n",
    "    fun=lambda w: objective_svm(w, train_features, train_labels, C),\n",
    "    # pass initial point\n",
    "    x0=np.ones(train_features.shape[-1]),\n",
    "    # pass function to evaluate gradient (in scipy.opt \"jac\" for jacobian)\n",
    "    jac=lambda w: d_objective_svm(w, train_features, train_labels, C),\n",
    "    # specify method to use\n",
    "    method=\"l-bfgs-b\"\n",
    ")\n",
    "print(res)\n",
    "w_svm = res.x\n",
    "\n",
    "## evaluation\n",
    "test_predictions = feature_fn(test_samples) @ w_svm\n",
    "train_predictions = feature_fn(train_samples) @ w_svm\n",
    "\n",
    "predicted_train_labels = np.ones(train_predictions.shape)\n",
    "predicted_train_labels[train_predictions < 0] = -1\n",
    "print(\"Train Accuracy: \", np.count_nonzero(predicted_train_labels == train_labels) / len(train_labels))\n",
    "\n",
    "predicted_test_labels = np.ones(test_predictions.shape)\n",
    "predicted_test_labels[test_predictions < 0] = -1\n",
    "print(\"Test Accuracy: \", np.count_nonzero(predicted_test_labels == test_labels) / len(test_labels))\n",
    "\n",
    "## plot train, contour, decision boundary and margins\n",
    "plt.figure()\n",
    "plt.title(\"Max Margin Solution (Predictions clipped at(-5, 5))\")\n",
    "x_plt_range = np.arange(-1.5, 2.5, 0.01)\n",
    "y_plt_range = np.arange(-1, 1.5, 0.01)\n",
    "plt_grid = np.stack(np.meshgrid(x_plt_range, y_plt_range), axis=-1)\n",
    "flat_plt_grid = np.reshape(plt_grid, [-1, 2])\n",
    "plt_grid_shape = plt_grid.shape[:2]\n",
    "\n",
    "pred_grid = np.reshape(feature_fn(flat_plt_grid) @ w_svm, plt_grid_shape)\n",
    "pred_grid = np.clip(pred_grid, -5, 5)\n",
    "#plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1.0, 0.0, 1.0], colors=[\"blue\", \"black\", \"orange\"])\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1, 0, 1], colors=('blue', 'black', 'orange'),\n",
    "            linestyles=('-',), linewidths=(2,))\n",
    "plt.contourf(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=10)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "s0 =plt.scatter(x=train_samples[train_labels == -1, 0], y=train_samples[train_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "s1 =plt.scatter(x=train_samples[train_labels == 1, 0], y=train_samples[train_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.ylim(-1, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Kernel Ridge Regression (7p)\n",
    "\n",
    "In this task, we are going to get familiar with the kernel method and perform Kernel Ridge Regression using Gaussian kernels.\n",
    "\n",
    "Work flow:\n",
    "- Load and plot data\n",
    "- Implement a function to get the Gaussian kernel vector\n",
    "- Implement a function to get the Gaussian kernel matrix\n",
    "- Implement a function to apply Kernel Ridge Regression\n",
    "- Select best model and see some result plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and plot data\n",
    "First, let us load and plot our data. Note that in this example the test data is not corrupted by noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set random seed to obtain reproducible results\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load training, validation and test datasets\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "x_valid = np.load('x_valid.npy')\n",
    "y_valid = np.load('y_valid.npy')\n",
    "\n",
    "x_test = np.load('x_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(dpi=200, tight_layout=True)\n",
    "plt.plot(x_train, y_train, 'ob', label='training data', zorder=20)\n",
    "plt.plot(x_valid, y_valid, 'or', label='validation data',zorder=10)\n",
    "plt.plot(x_test, y_test, 'ok', label='test data', alpha=0.3, zorder=0)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training, Validation and Testing datasets (1pt)\n",
    "In this homework, we are going to use a new type of dataset \"validation\" to help select our best hyperparameter and avoid overfitting. Please read the explanation below and understand the roles of training, validation and testing datasets respectively (1pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, validation, and test datasets are three distinct subsets of a larger dataset that are used in the process of building and evaluating machine learning models.\n",
    "\n",
    "The training dataset is the subset of the data that is used to train a machine learning model. In other words, it's the data that the model learns from. Typically, the training dataset makes up the majority of the available data.\n",
    "\n",
    "The validation dataset is used to evaluate the performance of a model during the training process. It's a subset of the data that is held back from the training process and used to tune the model's hyperparameters, such as learning rate, number of hidden layers, etc. The goal of the validation dataset is to help prevent overfitting, which is when the model memorizes the training data and performs poorly on new, unseen data.\n",
    "\n",
    "The test dataset is a completely separate subset of the data that is used to evaluate the final performance of a trained model. It represents real-world data that the model has never seen before and is used to estimate the model's accuracy on new, unseen data. The test dataset should only be used after the model has been trained and validated to prevent any bias in the evaluation.\n",
    "\n",
    "It's important to note that the three datasets should be independent and identically distributed (IID) to ensure that the model is able to generalize well to new data. In addition, the sizes of the datasets may vary depending on the size and complexity of the original dataset and the specific requirements of the machine learning problem at hand.\n",
    "\n",
    "(Note: In our homework, however, we are going to use a large and denoised test dataset which is for plotting convienence.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Kernels:\n",
    "Since Gaussian kernels are the most commonly used kernels, we will concentrate on Gaussian kernels in this notebook. Remember the definition of a Gaussian kernel (slide 55):\\\n",
    "\\begin{align*}\n",
    "    k(\\boldsymbol{x}, \\boldsymbol{x^*}) = \\exp\\left(-\\frac{||\\boldsymbol{x}-\\boldsymbol{x^*}||^2}{2\\sigma^2}\\right),\n",
    "\\end{align*}\n",
    "where $\\boldsymbol{x}, \\boldsymbol{x^*}$ are two $d$-dimensional data points. $ \\sigma $ is the bandwidth hyperparameter. Recall that any kernel $k(\\boldsymbol{x}, \\boldsymbol{x^*})$ returns a scalar which represents some kind of discrepancy measure between two data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Kernel Vector (3pts)\n",
    "Now we are going to implement a function to compute the kernel vector $\\boldsymbol k(\\boldsymbol x^*)$ (slide 59). Recall the definition: given $N$ training points $\\boldsymbol{x_i}, i=1,...,N$, and one additional query point $\\boldsymbol{x^*}$, $\\boldsymbol{k(x^*)}$ is defined as the $N$-dimensional vector whose $i$-th element is given by the kernel, evaluated at training point $\\boldsymbol{x_i}$ and the query point, i.e. $k(\\boldsymbol{x_i, x^*})$.\n",
    "\n",
    "Please finish the function below!\n",
    "\n",
    "Hints:\n",
    "- As we typically have $M > 1$ query points in practice, we would like to compute the corresponding $M$ kernel vectors in one function call, cf. the comments.\n",
    "- The computations we have to perform are the same as in `gaussian_kernel()`. However, we are now operating on more than one input vectors at once.\n",
    "- Avoid using loops! To this end, you may need to add additional dimensions to the input data and use broadcasting. Also, note that numpy operations such as `np.linalg.norm()` can operate on vector inputs and take an `axis`-argument!\n",
    "- Make sure your code works also for data with dimension larger than 1. You are going to need this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_vec(X_t: np.ndarray,\n",
    "                   X_q: np.ndarray,\n",
    "                   sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param X_t: N training inputs (shape: [N, d])\n",
    "    :param X_q: M query inputs (shape: [M, d])\n",
    "    :param sigma: bandwidth of the kernel (shape: scalar)\n",
    "    :return: M kernel vectors arranged as the columns of a matrix with shape [N, M]\n",
    "    \"\"\"\n",
    "    ############## Your code starts here ##############\n",
    "\n",
    "    ############### Your code ends here ###############\n",
    "\n",
    "    # assert the output shape being [N, M]\n",
    "    assert list(kernel_vectors.shape) == [X_t.shape[0], X_q.shape[0]]\n",
    "    return kernel_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function (RBF) features\n",
    "Now we use `get_kernel_vec()` to compute the kernel vectors w.r.t. our training inputs, using the test inputs as the query points. This allows us to plot the RBF features: we observe that at each training data point a Gaussian bump (the Gaussian kernel) is centered and that the bandwith parameter specifies the width of each bump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to test your code\n",
    "\n",
    "# Compute the kernel vectors w.r.t. the training inputs, using the test inputs as query points\n",
    "kernel_vectors = get_kernel_vec(x_train, x_test, 0.5)\n",
    "kernel_vectors2 = get_kernel_vec(x_train, x_test, 0.2)\n",
    "\n",
    "# Prepare plot\n",
    "plt.figure(dpi=200, tight_layout=True)\n",
    "\n",
    "# Plot the data\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_train, y_train, 'ro')\n",
    "plt.plot(x_test, y_test, 'k')\n",
    "plt.legend(['training data', 'ground_truth'])\n",
    "\n",
    "# Plot the kernel vectors\n",
    "plt.subplot(3,1,2)\n",
    "for kernel in kernel_vectors:\n",
    "    plt.plot(x_test, kernel, 'b')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "for kernel in kernel_vectors2:\n",
    "    plt.plot(x_test, kernel, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regression with Gaussian Kernels\n",
    "\n",
    "Let's go ahead and do regression using Gaussian kernels. Remember the regression equation (slide 67):\n",
    "\n",
    "\\begin{align*}\n",
    "    f(\\boldsymbol{x^*}) = \\boldsymbol{k}(\\boldsymbol{x^*})^T(\\boldsymbol{K} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{y},\n",
    "\\end{align*}\n",
    "where $ \\boldsymbol{k}(\\boldsymbol{x^*}) $ is the kernel vector, $ \\boldsymbol{K}$ is the kernel matrix and $ \\boldsymbol{y} $ are the target values from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Kernel Matrix (1pt)\n",
    "First, we have to compute the kernel matrix $\\boldsymbol K$ (slide 59). Recall the definition: $[\\boldsymbol K]_{ij} = k(\\boldsymbol x_i, \\boldsymbol x_j)$ with our training inputs $\\boldsymbol{x_i}, i=1,...,N$.\n",
    "\n",
    "Implement the function below (hint: re-use the `get_kernel_vec()` function)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_mat(X: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param X: N training inputs (shape [N, d])\n",
    "    :sigma: bandwidth of the kernel (scalar)\n",
    "    :return: the kernel matrix (shape [N, N])\n",
    "    \"\"\"\n",
    "    ############## Your code starts here ##############\n",
    "\n",
    "    ############### Your code ends here ###############\n",
    "    # assert the output shape being [N, N]\n",
    "    assert list(kernel_mat.shape) == [X.shape[0], X.shape[0]]\n",
    "    return kernel_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Kernel Regression Prediction (2pts)\n",
    "Now that we have finished the implementation of the kernel vector $\\boldsymbol{k(x^*)}$ and the kernel matrix $\\boldsymbol{K}$, we can compute the prediction $f(\\boldsymbol{x^*})$ in the function in below. Here we choose a fixed ridge factor $ \\lambda = 10^{-3}$.\n",
    "\n",
    "Hint: make sure you use numerically stable operations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_t: np.ndarray, y_t: np.ndarray,\n",
    "            X_q: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param X_t: N training inputs (shape [N, d])\n",
    "    :param y_t: N training targets (shape [N, 1])\n",
    "    :param X_q: M query inputs (shape [M, d])\n",
    "    :param sigma: bandwidth of the kernel (scalar)\n",
    "    :return: predicted values at X_q (shape [M, 1])\n",
    "    \"\"\"\n",
    "    k = get_kernel_vec(X_t, X_q, sigma) # shape [N, M]\n",
    "    K = get_kernel_mat(X_t, sigma) # shape [N, N]\n",
    "    ridge_factor = 1e-3\n",
    "    ############## Your code starts here ##############\n",
    "\n",
    "    ############### Your code ends here ###############\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "Once again, we will use the mean squared error to measure the training and test error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_target: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    :param y_target: true y values\n",
    "    :param y_pred: predicted y values\n",
    "    :return: MSE\n",
    "    \"\"\"\n",
    "    return np.sum((y_target-y_pred)**2)/y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "The Gaussian kernel method introduces a hyperparameter (the bandwidth $\\sigma$). We have to determine its value via model selection. Here, we use the hold-out method, i.e., we try different $\\sigma$ values and select the model which performs best on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [0.01, .1, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 9.5, 10]\n",
    "\n",
    "# compute train and validation errors for the different sigma values\n",
    "train_errors = []\n",
    "validate_errors = []\n",
    "for sigma in sigmas:\n",
    "    predict_train = predict(X_t=x_train, y_t=y_train,\n",
    "                              X_q=x_train, sigma=sigma)\n",
    "    predict_valid = predict(X_t=x_train, y_t=y_train,\n",
    "                             X_q=x_valid, sigma=sigma)\n",
    "    train_errors.append(mse(y_train, predict_train))\n",
    "    validate_errors.append(mse(y_valid, predict_valid))\n",
    "\n",
    "# determine best model\n",
    "min_valid_error_index = validate_errors.index(min(validate_errors))\n",
    "print(f'minimum validation error for sigma = {sigmas[min_valid_error_index]:.2f}')\n",
    "\n",
    "# plot the errors\n",
    "x_axis = [str(x) for x in sigmas]\n",
    "error_plot = plt.figure(dpi=200, tight_layout=True)\n",
    "plt.plot(x_axis, train_errors, 'b')\n",
    "plt.plot(x_axis, validate_errors, 'r')\n",
    "plt.plot(x_axis[min_valid_error_index], validate_errors[min_valid_error_index], 'gx')\n",
    "plt.xlabel( \"$\\sigma$\")\n",
    "plt.ylabel( \"MSE\" )\n",
    "plt.legend(['train error', 'validation error','min error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again observe the typical behavior: we have clear overfitting for small $ \\sigma $ values (high model complexity). For high $ \\sigma $ values (low model complexity) we observe underfitting. The best performing model is marked with a green 'x', as the minimum validation error is achieved for $ \\sigma = 3 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at predictions for different $ \\sigma $ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions for various bandwith parameters\n",
    "for sigma in [5.0,  3.0, 1.0 , 0.1]:\n",
    "    # compute predictions on train, test, and validation sets\n",
    "    y_pred_train = predict(X_t=x_train, y_t=y_train, X_q=x_train, sigma=sigma)\n",
    "    y_pred_valid = predict(X_t=x_train, y_t=y_train, X_q=x_valid, sigma=sigma)\n",
    "    y_pred_test = predict(X_t=x_train, y_t=y_train, X_q=x_test, sigma=sigma)\n",
    "\n",
    "    # print MSEs\n",
    "    print(f'sigma = {sigma:.2f}')\n",
    "    print(f'MSE train: {mse(y_train, y_pred_train):.2f}')\n",
    "    print(f'MSE valid: {mse(y_valid, y_pred_valid):.2f}')\n",
    "    print(f'MSE test : {mse(y_test, y_pred_test):.2f}')\n",
    "\n",
    "    # plot predictions\n",
    "    sigma_fig = plt.figure(dpi=75, tight_layout=True)\n",
    "    plt.plot(x_train, y_train, 'ro')\n",
    "    plt.plot(x_test, y_test, 'k')\n",
    "    plt.plot(x_test, y_pred_test, 'y')\n",
    "    plt.legend(['Training data', 'Ground-truth', '$\\sigma$ =' + str(sigma)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again observe that the model overfits for $ \\sigma < 3.0 $ and underfits for $\\sigma > 3.0$, with $\\sigma = 3.0$ achieving the minimum validation error.\n",
    "\n",
    "Note that the model prediction reverts back to $0$ far away from training data (where \"far away\" has to be understood relative to the bandwidth $\\sigma$). E.g., for $ \\sigma = 0.1 $, the model prediction reverts back to $0$ in the input region [-2.5, -1.5] as all training points are \"far away\" w.r.t. the small bandwith value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
